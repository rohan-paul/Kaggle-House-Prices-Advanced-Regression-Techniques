{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [
    {
     "ename": "ImportError",
     "evalue": "cannot import name 'sip' from 'PyQt5' (/home/paul/anaconda3/lib/python3.8/site-packages/PyQt5/__init__.py)",
     "output_type": "error",
     "traceback": [
      "\u001B[0;31m---------------------------------------------------------------------------\u001B[0m",
      "\u001B[0;31mImportError\u001B[0m                               Traceback (most recent call last)",
      "\u001B[0;32m<ipython-input-1-e96c43fd7d8e>\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mmatplotlib\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mpyplot\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mplt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mseaborn\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0msns\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpandasgui\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mshow\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandasgui/__init__.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[0;32m----> 1\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpandasgui\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mgui\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mshow\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      2\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0m__all__\u001B[0m \u001B[0;34m=\u001B[0m \u001B[0;34m[\u001B[0m\u001B[0;34m\"show\"\u001B[0m\u001B[0;34m]\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandasgui/gui.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m     10\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mPyQt5\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mQtCore\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mQt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     11\u001B[0m \u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m---> 12\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mpandasgui\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mstore\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mStore\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mPandasGuiDataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m     13\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpandasgui\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mutility\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mfix_ipython\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mfix_pyqt\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mget_logger\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mas_dict\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mdelete_datasets\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m     14\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpandasgui\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mwidgets\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mdataframe_explorer\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDataFrameExplorer\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;32m~/.local/lib/python3.8/site-packages/pandasgui/store.py\u001B[0m in \u001B[0;36m<module>\u001B[0;34m\u001B[0m\n\u001B[1;32m      3\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mas\u001B[0m \u001B[0mpd\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      4\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mpandas\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mDataFrame\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0;32m----> 5\u001B[0;31m \u001B[0;32mfrom\u001B[0m \u001B[0mPyQt5\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mQtCore\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mQtGui\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0mQtWidgets\u001B[0m\u001B[0;34m,\u001B[0m \u001B[0msip\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[0m\u001B[1;32m      6\u001B[0m \u001B[0;32mfrom\u001B[0m \u001B[0mPyQt5\u001B[0m\u001B[0;34m.\u001B[0m\u001B[0mQtCore\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mQt\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n\u001B[1;32m      7\u001B[0m \u001B[0;32mimport\u001B[0m \u001B[0mtraceback\u001B[0m\u001B[0;34m\u001B[0m\u001B[0;34m\u001B[0m\u001B[0m\n",
      "\u001B[0;31mImportError\u001B[0m: cannot import name 'sip' from 'PyQt5' (/home/paul/anaconda3/lib/python3.8/site-packages/PyQt5/__init__.py)"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train = pd.read_csv('input/house-prices-advanced-regression-techniques/train.csv')"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "First, I am getting a basic description of the data, to look quickly at the overall data to get some simple, easy-to-understand information. This is just to get a basic feel for the data. Using describe() function to get various summary statistics that exclude NaN values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now that I have got a general idea about your data set, it’s also a good idea to take a closer look at the data itself. With the help of the head() and tail() functions to check out the first and last lines of my DataFrame, respectively."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "df_train.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Get the overall concise summary of the DataFrame\n",
    "df_train.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Lets check what are the numerical features in the df_train dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features = df_train.select_dtypes(include=[np.number])\n",
    "numeric_features.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "numeric features head"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "numeric_features.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Now a general description of SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Now, as we will be predicting 'SalePrice' lets see description of that column\n",
    "df_train['SalePrice'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "For a regression problem, the most important thing to do is to understand the distribution\n",
    "of your target. If a target ranges between 1 and 10, and after training our model we get a\n",
    "mean absolute error of 5, we can tell that the error is large in this context.\n",
    "However, the same error for a target that ranges between 500,000 and 1,000,000 is\n",
    "negligible.\n",
    "\n",
    "- If you have numeric type dataset and want to visualize in histogram\n",
    "- then the seaborn histogram will help you. For this seaborn distplot function responsible to plot it.\n",
    "- On y-axis give the numeric dataset\n",
    "- On x-axis gives bins. It means distribute given dataset in a particular range and show in bars\n",
    "- e.g. bins= 18 and bins = 40"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.distplot(df_train['SalePrice'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "From the above distplot, the 'SalePrice' is dense between 100k and 250k, but there are larger outliers on the right side, i.e the expensive price range.\n",
    "\n",
    "Now, let’s have a look at the greater living area (square feet) against the sale price:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# first wo quickly view all the column names in the data\n",
    "# print(df_train.columns)\n",
    "# for above I could also use - data.columns\n",
    "# Checking - 'GrLivArea'\n",
    "data = pd.concat([df_train['SalePrice'], df_train['GrLivArea']], axis=1)\n",
    "data.plot.scatter(x='GrLivArea', y='SalePrice', ylim=(0, 800000), s=32)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " Its generally expected that larger living area comes with higher price. This chart shows thats generally true. But there are some 2–3 “cheap” houses with large living area\n",
    "Now lets explore the “TotalBsmtSF” column — Total square feet of the basement area."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Checking - 'TotalBsmtSF'\n",
    "data = pd.concat([df_train['SalePrice'], df_train['TotalBsmtSF']], axis=1)\n",
    "data.plot.scatter(x='TotalBsmtSF', y='SalePrice', ylim=(0, 800000))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "We can see that basement area indeed may have good predictive power for the model.\n",
    "\n",
    "Now lets check “OverallQual” — overall material and finish quality. This one may be more subjective feature.\n",
    "\n",
    "### Now list of variables that contain year information, because I will compare the difference between All years feature with SalePrice"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "year_feature = [feature for feature in numeric_features if 'Yr' in feature or 'Year' in feature]\n",
    "year_feature"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Checking - 'YearBuilt'\n",
    "data = pd.concat([df_train['SalePrice'], df_train['YearBuilt']], axis=1)\n",
    "data.plot.scatter(x = 'YearBuilt', y = 'SalePrice', ylim=(0, 800000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Checking - 'YearBuilt'\n",
    "data = pd.concat([df_train['SalePrice'], df_train['GarageYrBlt']], axis=1)\n",
    "data.plot.scatter(x = 'GarageYrBlt', y = 'SalePrice', ylim=(0, 800000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Checking - 'YearBuilt'\n",
    "data = pd.concat([df_train['SalePrice'], df_train['YearRemodAdd']], axis=1)\n",
    "data.plot.scatter(x = 'YearRemodAdd', y = 'SalePrice', ylim=(0, 800000))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Checking - 'YearBuilt'\n",
    "data = pd.concat([df_train['SalePrice'], df_train['YrSold']], axis=1)\n",
    "data.plot.scatter(x = 'YrSold', y = 'SalePrice', ylim=(0, 800000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Checking - TotRmsAbvGrd' i.e. total rooms above grade\n",
    "data = pd.concat([df_train['SalePrice'], df_train['TotRmsAbvGrd']], axis=1)\n",
    "data.plot.scatter(x = 'TotRmsAbvGrd', y = 'SalePrice', ylim=(0, 800000))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "data = pd.concat([df_train['SalePrice'], df_train['OverallQual']], axis=1)\n",
    "f, ax = plt.subplots(figsize=(14, 8))\n",
    "fig = sns.boxplot(x='OverallQual', y='SalePrice', data=data)\n",
    "fig.axis(ymin= 0, ymax=800000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "# Now lets view the top 8 correlated features with the sale price:\n",
    "corr_mat = df_train.corr()\n",
    "f, ax = plt.subplots(figsize=(12, 9))\n",
    "sns.heatmap(corr_mat, vmax=0.8, square=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "k = 9 # number of variables for the heatmap\n",
    "cols = corr_mat.nlargest(k, 'SalePrice')['SalePrice'].index\n",
    "f, ax = plt.subplots(figsize=(14,10))\n",
    "sns.heatmap(df_train[cols].corr(), vmax=0.8, square=True, annot=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now a simple Feature Engineering - of finding out the top-most correlated columns with SalesPrice. \n",
    "Later while in our Model will use only these top-most important features to predict SalePrices."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Feature Engineering - Find out the most correlated columns with SalesPrice and use only those columns in modelNow the big beautiful scatter plot with a hell of lot of visual data in a single plot"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "top_correlations = df_train.corr()\n",
    "top_feature_columns = top_correlations['SalePrice'][top_correlations['SalePrice'].values > 0.2].index.values\n",
    "top_feature_columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now another heatmap with ONLY the top 10 correlated features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Handling Missing Values for 19 features which have missing values mentioned above\n",
    "df_train['GarageYrBlt'] = df_train['GarageYrBlt'].fillna(0)\n",
    "# filling in missing GarageYrBuilt values with zeros. But this may not be the most \n",
    "# logical approach. Refer to this discussion below.\n",
    "# https://www.kaggle.com/c/house-prices-advanced-regression-techniques/discussion/60143\n",
    "\n",
    "heat_map_with_top_correlated_features = np.append(top_feature_columns[-10:], np.array(['SalePrice']))\n",
    "pearson_correlation_coefficients = np.corrcoef(df_train[heat_map_with_top_correlated_features[::-1]].T)\n",
    "plt.figure(figsize=(16,16))\n",
    "sns.set(font_scale=1)\n",
    "with sns.axes_style('white'):\n",
    "    sns.heatmap(pearson_correlation_coefficients, yticklabels=heat_map_with_top_correlated_features[::-1], xticklabels=heat_map_with_top_correlated_features[::-1], fmt='.2f', annot_kws={'size': 10}, annot=True, square=True, cmap=None)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "sns.set()\n",
    "cols = ['SalePrice', 'OverallQual', 'GrLivArea', 'GarageCars', 'TotalBsmtSF', '1stFlrSF', 'FullBath', 'TotRmsAbvGrd']\n",
    "sns.pairplot(df_train[cols], height=4)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "Checking out missing data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "total = df_train.isnull().sum().sort_values(ascending=False)\n",
    "percent = (df_train.isnull().sum() / df_train.isnull().count()).sort_values(ascending=False)\n",
    "missing_data = pd.concat([total, percent], axis=1, keys=['Total', 'Percent'])\n",
    "missing_data.head(20)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "## Now the all important SalePrice prediction, Using Linear Regression\n",
    "\n",
    "## Loss function\n",
    "\n",
    "Given our Simple Linear Regression equation:\n",
    "\n",
    "$$Y = bX + a$$\n",
    "\n",
    "We can use the following cost function to find the coefficients:\n",
    "\n",
    "### Mean Squared Error (MSE) Cost Function\n",
    "\n",
    "The MSE is defined as:\n",
    "\n",
    "$$MSE = J(W) =  \\frac{1}{n} \\sum_{i=1}^{n} (y^{(i)} - h_w(x^{(i)}))^2$$\n",
    "\n",
    "where\n",
    "\n",
    "$$h_w(x) = g(w^Tx)$$\n",
    "\n",
    "### and Root Mean Squared Error\n",
    "\n",
    "$$RMSE = \\sqrt{(\\frac{1}{n})\\sum_{i=1}^{n}(y^{(i)} - h_w(x^{(i))}))^{2}}$$\n",
    "\n",
    "Mean Squared Error (MSE) is the workhorse of basic loss functions. For calculating MSE, I take the difference between my y value predictions and the ground truth, square it, and average it out across the whole dataset."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "def loss_func(y_predicted, y_actual):\n",
    "    squared_error = (y_predicted - y_actual) ** 2\n",
    "    sum_squared_error = squared_error.sum()\n",
    "    size = len(y_actual)\n",
    "    return 1/(2*size) * sum_squared_error"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = df_train['GrLivArea']\n",
    "y = df_train['SalePrice']\n",
    "\n",
    "x = (x - x.mean()) / x.std()\n",
    "x = np.c_[np.ones(x.shape[0]), x]\n",
    "x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "##### Now lets define our Hypothesis variable, meaning the one to be predicted, and here we are predicting 'Sale Price'. And also the independent variable. Lets say first I want to make the independent variable as 'GrLivArea' which represents, \"Above grade (ground) living area square feet\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = df_train['GrLivArea']\n",
    "y = df_train['SalePrice']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " I will preprocess the data using the following formula for standardization\n",
    "\n",
    "$x′=(x−μ) / σ$\n",
    "\n",
    "The above formulae is called Z-score is one of the most popular methods to standardize data. It is equal to the original variable, minus its mean, divided by its standard deviation.\n",
    "Once the standardization is done, all the features will have a mean of zero, a standard deviation of one, and thus, the same scale.\n",
    "This process produces standard scores that represent the number of standard deviations above or below the mean that a specific observation falls. For instance, a standardized value of 2 indicates that the observation falls 2 standard deviations above the mean. This interpretation is true regardless of the type of variable that you standardize."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "x = (x - x.mean()) / x.std()\n",
    "print(x.shape)\n",
    "# (1460, ) i.e. it will be an array like\n",
    "# [0 1 2 3 4 5 6 7 8.... 1459]\n",
    "\n",
    "x = np.c_[np.ones(x.shape[0]), x]\n",
    "print(x.shape)\n",
    "# (1460, 2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Now below I am defining a basic Linear regression function\n",
    "\n",
    "class SimpleLinearRegression:\n",
    "\n",
    "    def get_predictions(self, X):\n",
    "        return np.dot(X, self._W)\n",
    "\n",
    "    def _get_gradient_descent_step(self, X, targets, learning_rate):\n",
    "        predictions = self.get_predictions(X)\n",
    "\n",
    "        error = predictions - targets\n",
    "        gradient = np.dot(X.T, error)/len(x)\n",
    "\n",
    "        # now update the W\n",
    "        self._W -= learning_rate * gradient\n",
    "\n",
    "\n",
    "    def fit(self, X, y, iterations_num=1000, learning_rate=0.01):\n",
    "            self._W = np.zeros(X.shape[1])\n",
    "\n",
    "            self._history_of_cost = []\n",
    "            self._w_history = [self._W]\n",
    "\n",
    "            for i in range(iterations_num):\n",
    "                predictions = self.get_predictions(X)\n",
    "                cost = loss_func(predictions, y)\n",
    "\n",
    "                self._history_of_cost.append(cost)\n",
    "\n",
    "                self._get_gradient_descent_step(x, y, learning_rate)\n",
    "\n",
    "                self._w_history.append(self._W.copy())\n",
    "\n",
    "            return self\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "house_price_linear_result = SimpleLinearRegression()\n",
    "house_price_linear_result.fit(x, y, iterations_num=1000, learning_rate=0.01)\n",
    "print(house_price_linear_result._W)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": false,
    "jupyter": {
     "outputs_hidden": false
    },
    "pycharm": {
     "name": "#%%\n"
    }
   },
   "outputs": [],
   "source": [
    "# Now plot the Cost function\n",
    "plt.title('Kaggle House Price Cost Function')\n",
    "plt.xlabel('No of Iterations')\n",
    "plt.ylabel('House Price Cost')\n",
    "plt.plot(house_price_linear_result._history_of_cost)\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}